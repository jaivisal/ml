import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder

def preprocess_data(data, label_column):
    # Handle categorical variables and missing data
    data = data.copy()

    # Fill missing values (numerical features)
    for col in data.columns:
        if data[col].dtype == 'object':
            data[col] = data[col].fillna(data[col].mode()[0])
        else:
            data[col] = data[col].fillna(data[col].median())
    
    # Convert categorical features to numerical using one-hot encoding
    data = pd.get_dummies(data, drop_first=True)

    # Extract features (X) and labels (Y)
    X = data.drop(label_column, axis=1).values
    Y = data[label_column].values

    return X, Y

# Define the sigmoid function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Define the logistic regression model with L2 regularization
def model(X, Y, learning_rate, iterations, lambda_reg):
    m = X.shape[1]
    n = X.shape[0]
    W = np.zeros((n, 1))
    B = 0
    cost_list = []

    for i in range(iterations):
        Z = np.dot(W.T, X) + B
        A = sigmoid(Z)
        
        # Cost function with L2 regularization
        cost = -(1/m) * np.sum(Y * np.log(A + 1e-8) + (1-Y) * np.log(1-A + 1e-8)) + (lambda_reg/(2*m)) * np.sum(np.square(W))
        
        # Gradient Descent with L2 regularization
        dW = (1/m) * np.dot(A-Y, X.T) + (lambda_reg/m) * W.T
        dB = (1/m) * np.sum(A - Y)
        
        W = W - learning_rate * dW.T
        B = B - learning_rate * dB
        
        # Keep track of the cost function value
        cost_list.append(cost)
        if i % (iterations/10) == 0:
            print("Cost after", i, "iterations:", cost)
    
    return W, B, cost_list

# Define the accuracy function
def accuracy(X, Y, W, B):
    Z = np.dot(W.T, X) + B
    A = sigmoid(Z)
    A = A > 0.5
    A = np.array(A, dtype='int64')
    acc = (1 - np.sum(np.abs(A - Y)) / Y.shape[1]) * 100
    print("Accuracy of the model is:", round(acc, 2), "%")

# Main function to run the logistic regression model
def run_logistic_regression(file_path, label_column, iterations=100000, learning_rate=0.005, lambda_reg=0.1):
    # Load the dataset
    data = pd.read_csv(file_path)

    # Preprocess the dataset
    X, Y = preprocess_data(data, label_column)

    # Perform the train-test split (80% training, 20% testing)
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

    # Standardize features by removing the mean and scaling to unit variance
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train).T
    X_test = scaler.transform(X_test).T

    Y_train = Y_train.reshape(1, X_train.shape[1])
    Y_test = Y_test.reshape(1, X_test.shape[1])

    print("Shape of X_train: ", X_train.shape)
    print("Shape of Y_train: ", Y_train.shape)
    print("Shape of X_test: ", X_test.shape)
    print("Shape of Y_test: ", Y_test.shape)

    # Train the model using the training set
    W, B, cost_list = model(X_train, Y_train, learning_rate, iterations, lambda_reg)

    # Plot the cost function to see if it decreases
    plt.plot(np.arange(iterations), cost_list)
    plt.xlabel("Iterations")
    plt.ylabel("Cost")
    plt.title("Cost vs Iterations")
    plt.show()

    # Test the model accuracy using the test set
    accuracy(X_test, Y_test, W, B)

# Example usage:
# Replace 'Titanic-DATASET.csv' with your dataset file path and 'Survived' with the label column name
run_logistic_regression("DIABETICS DATASET.csv", "Outcome")
